{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install bitsandbytes transformers peft accelerate trl datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from transformers import LlamaForCausalLM\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive를 마운트하여 파일에 접근할 수 있도록 설정\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train = pd.read_csv('/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/pymupdf4llm/finetuning_prompt_cerebro.csv')\n",
    "\n",
    "# 모델 및 토크나이저 설정\n",
    "model_id = \"I-BRICKS/Cerebro_BM_solar_v01\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='eager',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "dataset = Dataset.from_dict({\"text\": train['prompt'].values})\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# 모델 준비 (k-bit 학습을 위해)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config_dict = {\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_bias': \"none\",\n",
    "    'lora_task_type': \"CAUSAL_LM\",\n",
    "    'target_modules': [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\", \"v_proj\"]\n",
    "}\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_config_dict['lora_alpha'],\n",
    "    lora_dropout=lora_config_dict['lora_dropout'],\n",
    "    r=lora_config_dict['lora_r'],\n",
    "    bias=lora_config_dict['lora_bias'],\n",
    "    task_type=lora_config_dict['lora_task_type'],\n",
    "    target_modules=lora_config_dict['target_modules']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정\n",
    "train_param = {\n",
    "    'num_train_epochs': 2,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'optim': \"adamw_torch\",\n",
    "    'save_steps': 300,\n",
    "    'logging_steps': 10,\n",
    "    'learning_rate': 0.0002,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1,\n",
    "    'warmup_ratio': 0.06,\n",
    "    'group_by_length': False,\n",
    "    'lr_scheduler_type': 'cosine'\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/pymupdf4llm/finetune/\",\n",
    "    num_train_epochs=train_param['num_train_epochs'],\n",
    "    per_device_train_batch_size=train_param['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=train_param['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=train_param['gradient_accumulation_steps'],\n",
    "    optim=train_param['optim'],\n",
    "    save_steps=train_param['save_steps'],\n",
    "    logging_steps=train_param['logging_steps'],\n",
    "    learning_rate=train_param['learning_rate'],\n",
    "    weight_decay=train_param['weight_decay'],\n",
    "    max_grad_norm=train_param['max_grad_norm'],\n",
    "    warmup_ratio=train_param['warmup_ratio'],\n",
    "    group_by_length=train_param['group_by_length'],\n",
    "    lr_scheduler_type=train_param['lr_scheduler_type']\n",
    ")\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "trainer.model.save_pretrained(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

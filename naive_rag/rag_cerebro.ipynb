{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install -qU transformers sentence-transformers datasets bitsandbytes\n",
    "!pip install pymupdf4llm==0.0.10\n",
    "!pip install orjson==3.9.15\n",
    "!pip install -qU langchain chromadb langchain-chroma langchain_community\n",
    "!pip install langchain-huggingface\n",
    "!pip install peft # LoRA 튜닝을 위한 라이브러리\n",
    "!pip install accelerate\n",
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pymupdf4llm\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive를 마운트하여 파일에 접근할 수 있도록 설정\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일을 처리하여 문서 청크로 분할하는 함수\n",
    "def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n",
    "    \"\"\"\n",
    "    PDF 파일을 읽어와서 마크다운으로 변환한 후,\n",
    "    주어진 청크 크기와 오버랩을 기준으로 문서 청크로 분할합니다.\n",
    "    \"\"\"\n",
    "    # PDF 파일을 마크다운으로 변환\n",
    "    pdf = pymupdf4llm.to_markdown(file_path)\n",
    "    pdf = re.sub(r'(?<!\\n)\\n\\n(?!\\n)', '\\n', pdf)  # 약간의 전처리\n",
    "    doc = [Document(page_content=pdf, metadata={\"source\": file_path})]\n",
    "    \n",
    "    # PDF를 청크로 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(doc)\n",
    "    return chunks\n",
    "\n",
    "# 문서를 벡터 데이터베이스로 변환하는 함수\n",
    "def create_vector_db(document, embeddings, file_path):\n",
    "    \"\"\"\n",
    "    주어진 문서를 벡터 데이터베이스로 변환하여 저장합니다.\n",
    "    \"\"\"\n",
    "    db = Chroma.from_documents(documents=document, embedding=embeddings, persist_directory=file_path)\n",
    "    return db\n",
    "\n",
    "# 모든 PDF 파일을 처리하고 벡터 데이터베이스를 생성하는 함수\n",
    "def create_pdf_databases(base_directory, embeddings, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    주어진 디렉토리의 모든 PDF 파일을 처리하여 벡터 데이터베이스를 생성합니다.\n",
    "    \"\"\"\n",
    "    pdf_databases = {}\n",
    "    for p in tqdm(os.listdir(base_directory), desc=\"Creating Vectordb for each PDF\"):\n",
    "        print(p)\n",
    "        doc = process_pdf(base_directory + p, chunk_size, chunk_overlap)\n",
    "\n",
    "        # 벡터 데이터베이스 저장 경로 설정\n",
    "        if 'train_source' in base_directory:\n",
    "            file_path = base_directory[:base_directory.find('train_source/')]\n",
    "        elif 'test_source' in base_directory:\n",
    "            file_path = base_directory[:base_directory.find('test_source/')]\n",
    "\n",
    "        vectordb_path = file_path + 'pymupdf4llm/' + p[:p.find('.pdf')]\n",
    "\n",
    "        # 기존 벡터 데이터베이스가 있는지 확인하고 로드\n",
    "        if os.path.exists(vectordb_path):\n",
    "            print(f\"Vector DB already exists at {vectordb_path}, loading existing DB...\")\n",
    "            vectordb = Chroma(persist_directory=vectordb_path, embedding_function=embeddings)\n",
    "        else:\n",
    "            print(f\"Creating new Vector DB at {vectordb_path}...\")\n",
    "            vectordb = create_vector_db(doc, embeddings, vectordb_path)\n",
    "\n",
    "        pdf_databases[unicodedata.normalize('NFC', p.split('.pdf')[0])] = {\n",
    "            'db': vectordb,\n",
    "            'doc': doc\n",
    "        }\n",
    "\n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 모델 설정\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "bge_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# PDF 데이터베이스 생성\n",
    "base_directory = '/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/test_source/'\n",
    "pdf_databases = create_pdf_databases(base_directory, bge_embeddings, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 파이프라인 설정 함수\n",
    "def setup_llm_pipeline():\n",
    "    \"\"\"\n",
    "    LoRA 튜닝된 LLM 모델과 함께 text-generation 파이프라인을 설정합니다.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model_id = \"I-BRICKS/Cerebro_BM_solar_v01\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    adapter_name = \"/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/pymupdf4llm/finetune/checkpoint-248\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_name, is_trainable=False)\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=512,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf\n",
    "\n",
    "hf = setup_llm_pipeline()\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 문서 내용을 포맷팅하는 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 질문에 대한 답변을 생성하는 과정\n",
    "sub_list = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Answering Questions\"):\n",
    "    retriever = pdf_databases[unicodedata.normalize('NFC', row['Source'])]['db'].as_retriever(search_kwargs={\"k\":5})\n",
    "\n",
    "    # 질문에 대한 답변을 생성하기 위해 압축기와 검색기를 설정합니다.\n",
    "    compressor = HuggingFaceCrossEncoder(model=reranker_model, top_n=2)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "\n",
    "    # 질문에 대한 답변을 생성하기 위한 프롬프트 템플릿 설정\n",
    "    template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    주어진 질문에만 답변하세요. 문장으로 답변해주세요. 답변할 때 질문의 주어를 써주세요.\n",
    "    답변:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG 체인 설정\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": compression_retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | hf\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    question = row['Question']\n",
    "    print('------------------------')\n",
    "    print('질문: ', question)\n",
    "    print('------------------------')\n",
    "    answer = rag_chain.invoke(question).strip()\n",
    "    print('답변: ', answer)\n",
    "\n",
    "    sub_list.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성\n",
    "submission_df = pd.DataFrame({\n",
    "    'Question': test_df['Question'],\n",
    "    'Answer': sub_list\n",
    "})\n",
    "submission_df.to_csv('/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/submission.csv', index=False)\n",
    "\n",
    "# 현재 날짜와 시간으로 파일 이름 생성\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "file_name = f'submission_{formatted_time}.csv'\n",
    "\n",
    "# 파일을 UTF-8 인코딩으로 저장\n",
    "submission_df.to_csv(file_name, index=False, encoding='UTF-8')\n",
    "\n",
    "print(f\"파일이 저장되었습니다: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

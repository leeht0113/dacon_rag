{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install -qU pymupdf4llm transformers sentence-transformers datasets bitsandbytes\n",
    "!pip install orjson==3.9.15\n",
    "!pip install -qU langchain chromadb langchain-chroma langchain_community\n",
    "!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import pymupdf4llm\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 드라이브 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일을 처리하고 분할하는 함수\n",
    "def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n",
    "    \"\"\"PDF 파일을 읽고 지정된 크기로 분할\"\"\"\n",
    "    pdf = pymupdf4llm.to_markdown(file_path)  # PDF 파일을 마크다운 형식으로 변환\n",
    "    pdf = re.sub(r'(?<!\\n)\\n\\n(?!\\n)', '\\n', pdf)  # 줄바꿈 관련 전처리\n",
    "    doc = [Document(page_content=pdf, metadata={\"source\": file_path})]  # Document 객체 생성\n",
    "    splitter = RecursiveCharacterTextSplitter(  # 문서 분할 설정\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_documents(doc)  # 문서 분할\n",
    "    return chunk_temp  # 분할된 문서 반환\n",
    "\n",
    "# 벡터 DB 생성 함수\n",
    "def create_vector_db(document, embeddings, file_path):\n",
    "    \"\"\"문서로부터 벡터 DB 생성\"\"\"\n",
    "    db = Chroma.from_documents(\n",
    "        documents=document,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=file_path\n",
    "    )\n",
    "    return db\n",
    "\n",
    "# PDF 파일들로부터 벡터 DB 생성 함수\n",
    "def create_pdf_databases(base_directory, embeddings, chunk_size, chunk_overlap):\n",
    "    \"\"\"PDF 파일들로부터 각각의 벡터 DB를 생성하고 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    for p in tqdm(os.listdir(base_directory), desc=\"Creating Vectordb for each PDF\"):\n",
    "        print(p)\n",
    "        doc = process_pdf(base_directory + p, chunk_size, chunk_overlap)\n",
    "\n",
    "        # 파일 경로 설정 (train_source 또는 test_source)\n",
    "        if base_directory.split('/')[-2] == 'train_source':\n",
    "            file_path = base_directory[:base_directory.find('train_source/')]\n",
    "        elif base_directory.split('/')[-2] == 'test_source':\n",
    "            file_path = base_directory[:base_directory.find('test_source/')]\n",
    "\n",
    "        # 벡터 DB 저장 경로 설정\n",
    "        vectordb_path = file_path + 'pymupdf4llm/' + p[:p.find('.pdf')]\n",
    "\n",
    "        # 벡터 DB가 이미 존재하는 경우 로드, 없으면 새로 생성\n",
    "        if os.path.exists(vectordb_path):\n",
    "            print(f\"Vector DB already exists at {vectordb_path}, loading existing DB...\")\n",
    "            vectordb = Chroma(persist_directory=vectordb_path, embedding_function=embeddings)\n",
    "        else:\n",
    "            print(f\"Creating new Vector DB at {vectordb_path}...\")\n",
    "            vectordb = create_vector_db(doc, embeddings, vectordb_path)\n",
    "\n",
    "        # 데이터베이스와 문서 저장\n",
    "        pdf_databases[unicodedata.normalize('NFC', p.split('.pdf')[0])] = {\n",
    "            'db': vectordb,\n",
    "            'doc': doc\n",
    "        }\n",
    "\n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace 임베딩 모델 설정\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "bge_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# PDF 파일들이 저장된 디렉토리 경로\n",
    "base_directory = '/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/train_source/'\n",
    "\n",
    "# PDF 파일들로부터 벡터 DB 생성\n",
    "pdf_databases = create_pdf_databases(base_directory, bge_embeddings, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# 학습용 데이터 CSV 파일 로드\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/train.csv')\n",
    "\n",
    "# 문서 재정렬을 위한 HuggingFace 모델 설정\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 검색된 문서의 내용을 하나의 문단으로 합치는 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 설정\n",
    "model_id = \"I-BRICKS/Cerebro_BM_solar_v01\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt_list = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Creating prompt template\"):\n",
    "    # 각 질문에 대해 상위 5개의 문서 검색\n",
    "    retriever = pdf_databases[unicodedata.normalize('NFC', row['Source'])]['db'].as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # 문서 압축을 위한 CrossEncoder 설정\n",
    "    compressor = CrossEncoderReranker(model=reranker_model, top_n=2)\n",
    "\n",
    "    # 문서 압축 검색기 초기화\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "\n",
    "    # 질문에 맞는 문서 검색\n",
    "    docs = compression_retriever.invoke(row['Question'])\n",
    "    context = format_docs(docs)\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "\n",
    "    # 프롬프트 템플릿 작성\n",
    "    template = f\"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    주어진 질문에만 답변하세요. 문장으로 답변해주세요. 답변할 때 질문의 주어를 써주세요.\n",
    "    답변: {answer}{tokenizer.eos_token}\n",
    "    \"\"\"\n",
    "    prompt_list.append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 프롬프트 리스트를 데이터프레임에 추가\n",
    "train_df['prompt'] = prompt_list\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "train_df.to_csv('/content/drive/MyDrive/재정정보 AI 검색 알고리즘 경진대회/pymupdf4llm/finetuning_prompt_cerebro.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
